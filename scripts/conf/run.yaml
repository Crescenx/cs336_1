run:
  type: "tiny"
  total_token: 256
  batch_size: 1
  use_cosine: true
  use_grad_clip: true
  load_checkpoint: false
  device: "cpu"
  eval_interval: 1
  checkpointing:
    save_interval: 1
    path: "checkpoints/"
model:
  vocab_size:
    tiny: 10000
    owt: 32000
  context_length: 256
  d_model: 512
  d_ff: 1344
  num_layers: 4
  num_heads: 16
  theta: 10000
optimizer:
  lr: 0.001
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.01
  eps: 1e-8
  cosine:
    max_lr: 0.001
    min_lr: 0.0001
    warmup: 0.5
    cycles: 1.0
  grad_clip: 1.0